{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bfcf3aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "sentence_model=SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
    "kw_model = KeyBERT(model=sentence_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1ee4f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('programacion', 0.5071),\n",
       " ('ingeniero', 0.41),\n",
       " ('sistemas', 0.3792),\n",
       " ('experto', 0.3747),\n",
       " ('lenguajes', 0.3598),\n",
       " ('python', 0.334),\n",
       " ('relacionales', 0.2687)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = \"\"\"  \n",
    "        Ingeniero de sistemas con enf√°sis y bases de datos relacionales y no relacionales. Domino lenguajes de programacion como \n",
    "        python, c++ y Java como experto\n",
    "      \"\"\"\n",
    "keywords = kw_model.extract_keywords(doc,top_n=7,keyphrase_ngram_range=(1, 1))\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5eb98f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14425/582875280.py:3: UserWarning: \n",
      "The dash_core_components package is deprecated. Please replace\n",
      "`import dash_core_components as dcc` with `from dash import dcc`\n",
      "  import dash_core_components as dcc\n",
      "/tmp/ipykernel_14425/582875280.py:4: UserWarning: \n",
      "The dash_html_components package is deprecated. Please replace\n",
      "`import dash_html_components as html` with `from dash import html`\n",
      "  import dash_html_components as html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f507b5b9bd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)\n",
      "\u001b[1;32m~/.local/lib/python3.10/site-packages/keybert/_model.py\u001b[0m in \u001b[0;36mextract_keywords\u001b[1;34m(\n",
      "    self=<keybert._model.KeyBERT object>,\n",
      "    docs=None,\n",
      "    candidates=None,\n",
      "    keyphrase_ngram_range=(1, 1),\n",
      "    stop_words='english',\n",
      "    top_n=20,\n",
      "    min_df=1,\n",
      "    use_maxsum=False,\n",
      "    use_mmr=False,\n",
      "    diversity=0.5,\n",
      "    nr_candidates=20,\n",
      "    vectorizer=None,\n",
      "    highlight=False,\n",
      "    seed_keywords=None,\n",
      "    doc_embeddings=None,\n",
      "    word_embeddings=None\n",
      ")\u001b[0m\n",
      "\u001b[0;32m    152\u001b[0m                     \u001b[0mmin_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    153\u001b[0m                     \u001b[0mvocabulary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m--> 154\u001b[1;33m                 ).fit(docs)\n",
      "\u001b[0m        \u001b[1;36mglobal\u001b[0m \u001b[0;36mfit\u001b[0m \u001b[1;34m= \u001b[1;36mundefined\u001b[0m\u001b[0m\u001b[1;34m\n",
      "        \u001b[0m\u001b[0;36mdocs\u001b[0m \u001b[1;34m= None\u001b[0m\n",
      "\u001b[0;32m    155\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    156\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32m~/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(\n",
      "    self=CountVectorizer(stop_words='english'),\n",
      "    raw_documents=None,\n",
      "    y=None\n",
      ")\u001b[0m\n",
      "\u001b[0;32m   1289\u001b[0m         \"\"\"\n",
      "\u001b[0;32m   1290\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_warn_for_unused_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m-> 1291\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m        \u001b[0;36mself.fit_transform\u001b[0m \u001b[1;34m= <bound method CountVectorizer.fit_transform of CountVectorizer(stop_words='english')>\u001b[0m\u001b[1;34m\n",
      "        \u001b[0m\u001b[0;36mraw_documents\u001b[0m \u001b[1;34m= None\u001b[0m\n",
      "\u001b[0;32m   1292\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m   1293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32m~/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(\n",
      "    self=CountVectorizer(stop_words='english'),\n",
      "    raw_documents=None,\n",
      "    y=None\n",
      ")\u001b[0m\n",
      "\u001b[0;32m   1336\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m   1337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m-> 1338\u001b[1;33m         \u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m        \u001b[0;36mvocabulary\u001b[0m \u001b[1;34m= \u001b[1;36mundefined\u001b[0m\u001b[0m\u001b[1;34m\n",
      "        \u001b[0m\u001b[0;36mX\u001b[0m \u001b[1;34m= \u001b[1;36mundefined\u001b[0m\u001b[0m\u001b[1;34m\n",
      "        \u001b[0m\u001b[0;36mself._count_vocab\u001b[0m \u001b[1;34m= <bound method CountVectorizer._count_vocab of CountVectorizer(stop_words='english')>\u001b[0m\u001b[1;34m\n",
      "        \u001b[0m\u001b[0;36mraw_documents\u001b[0m \u001b[1;34m= None\u001b[0m\u001b[1;34m\n",
      "        \u001b[0m\u001b[0;36mself.fixed_vocabulary_\u001b[0m \u001b[1;34m= False\u001b[0m\n",
      "\u001b[0;32m   1339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m   1340\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32m~/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(\n",
      "    self=CountVectorizer(stop_words='english'),\n",
      "    raw_documents=None,\n",
      "    fixed_vocab=False\n",
      ")\u001b[0m\n",
      "\u001b[0;32m   1205\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_make_int_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m   1206\u001b[0m         \u001b[0mindptr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m-> 1207\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m        \u001b[0;36mdoc\u001b[0m \u001b[1;34m= \u001b[1;36mundefined\u001b[0m\u001b[0m\u001b[1;34m\n",
      "        \u001b[0m\u001b[0;36mraw_documents\u001b[0m \u001b[1;34m= None\u001b[0m\n",
      "\u001b[0;32m   1208\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m   1209\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "from jupyter_dash import JupyterDash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output\n",
    "from keybert import KeyBERT\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# Load Data\n",
    "sentence_model=SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
    "kw_model = KeyBERT(model=sentence_model)\n",
    "with open('preps.txt','r') as pfile:\n",
    "    prep=[i.replace('\\n','') for i in pfile.readlines()]\n",
    "\n",
    "# Build App\n",
    "app = JupyterDash(__name__)\n",
    "app.layout = html.Div([\n",
    "    html.Div([dcc.Textarea(id='profile',\n",
    "                           style={'width':'50%','height':'50%'}\n",
    "                          ),\n",
    "    dcc.Loading(id='loading-tags',\n",
    "                children=[html.Div(dcc.Dropdown(id='labels',multi=True))],\n",
    "                type='circle'\n",
    "    )\n",
    "    ])\n",
    "    \n",
    "])\n",
    "# Define callback to update graph\n",
    "@app.callback(Output('labels','value'),\n",
    "              Output('labels','options'),\n",
    "              [Input('profile','value')])\n",
    "def show(text_profile):\n",
    "    keywords = kw_model.extract_keywords(text_profile,top_n=20,keyphrase_ngram_range=(1, 1))\n",
    "    values=[i[0] for i in keywords  if i[0] not in prep]\n",
    "    options=[{'label':i[0],'value':i[0] } for i in keywords]\n",
    "    \n",
    "    \n",
    "    return values,options\n",
    "\n",
    "# Run app and display result inline in the notebook\n",
    "app.run_server(mode='inline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd3c091",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
